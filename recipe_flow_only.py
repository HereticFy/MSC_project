# -*- coding: utf-8 -*-
"""recipe_flow_only.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W86AZHIOKDPwqwAq5o2hca4eXfyzlzI1
"""

!pip install Sentencepiece
!pip install transformers

from google.colab import drive
drive.mount('/content/drive')

#from transformers import XLNetForSequenceClassification
import pandas as pd
from glob import glob
import os
import random
import torch
from torch.utils import data
import numpy as np
import shutil
import os
from tqdm import tqdm

from transformers import BertTokenizer
import torch.utils.data as Data
from transformers import BertForSequenceClassification
from torch.optim import AdamW

a = pd.read_csv("/content/drive/MyDrive/recipe_data/edge_test_data.txt", header=None)
b = a[8].tolist()
max(b)
#len(encoded_dict['input_ids'])

a

c = "while the chicken is cooking get other frying pan out and add the scoop of butter let it melt then add the chopped mushrooms then crush or finley chop the garlic and put in with the mushrooms ( do not drain of the butter that is the garlic sauce put all the food on each plate and serve !"
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
encoded_dict = tokenizer(c)

TRAIN_FLOW_PATH = "/content/drive/MyDrive/recipe_data/edge_training_data.txt"
TEST_FLOW_PATH = "/content/drive/MyDrive/recipe_data/edge_test_data.txt"


edge_train_csv = pd.read_csv(TRAIN_FLOW_PATH, header=None)
edge_test_csv = pd.read_csv(TEST_FLOW_PATH, header=None)


# number of different types of labels
#EDGE_LABELS_NUMBER = len(EDGE_LABEL_LIST)

TRAIN_SENTENCES_LIST = edge_train_csv[8].tolist()

TRAIN_LABELS = edge_train_csv[2].tolist()

# There are only 2 labels : relate or not-relate
for i in range(len(TRAIN_LABELS)):
  # 1 and 12 represent not related
  if "not-relate" in TRAIN_LABELS[i]:
    TRAIN_LABELS[i] = 0
  elif "(e1,e2)" in TRAIN_LABELS[i]:
    TRAIN_LABELS[i] = 1
  else:
    TRAIN_LABELS[i] = 2

train_e1 = edge_train_csv[6].tolist()
train_e2 = edge_train_csv[7].tolist()
TRAIN_ENTITIES = []
for i in range(len(train_e1)):
    TRAIN_ENTITIES.append(train_e1[i] + " " + train_e2[i])

TEST_SENTENCES_LIST = edge_test_csv[8].tolist()
TEST_LABELS = edge_test_csv[2].tolist()

for i in range(len(TEST_LABELS)):
  # 1 and 12 represent not related
  if "not-relate" in TEST_LABELS[i]:
    TEST_LABELS[i] = 0
  elif "(e1,e2)" in TEST_LABELS[i]:
    TEST_LABELS[i] = 1
  else:
    TEST_LABELS[i] = 2


test_e1 = edge_test_csv[6].tolist()
test_e2 = edge_test_csv[7].tolist()
TEST_ENTITIES = []
for i in range(len(test_e1)):
    TEST_ENTITIES.append(test_e1[i] + " " + test_e2[i] + ".")

NUMBER_CLASSES = 3





def convert(entities, sentences, target): # name_list, sentence_list, label_list
    input_ids, token_type_ids, attention_mask = [], [], []
    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
    for i in range(len(sentences)):
        encoded_dict = tokenizer.encode_plus(
            entities[i] + " " + sentences[i],        # 输入文本
            add_special_tokens = True,      # 添加 '[CLS]' 和 '[SEP]'
            max_length = 128,             # 不够填充
            padding = 'max_length',
            #truncation = True,             # 太长截断
            return_tensors = 'pt',         # 返回 pytorch tensors 格式的数据
        )
        input_ids.append(encoded_dict['input_ids'])
        token_type_ids.append(encoded_dict['token_type_ids'])
        attention_mask.append(encoded_dict['attention_mask'])

    input_ids = torch.cat(input_ids, dim=0)
    token_type_ids = torch.cat(token_type_ids, dim=0)
    attention_mask = torch.cat(attention_mask, dim=0)

    input_ids = torch.LongTensor(input_ids)
    token_type_ids = torch.LongTensor(token_type_ids)
    attention_mask = torch.LongTensor(attention_mask)
    target = torch.LongTensor(target)

    return input_ids, token_type_ids, attention_mask, target

input_ids, token_type_ids, attention_mask, target = convert(TRAIN_ENTITIES, TRAIN_SENTENCES_LIST, TRAIN_LABELS)

import torch.utils.data as Data
from transformers import XLNetForSequenceClassification
from torch.optim import AdamW

def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten() # [3, 5, 8, 1, 2, ....]
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)


def train_eval(model, input_ids, token_type_ids, attention_mask, labels):
    batch_size = 100
    train_data = Data.TensorDataset(input_ids, token_type_ids, attention_mask, labels)
    train_dataloader = Data.DataLoader(train_data, batch_size=batch_size, shuffle=True)

    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
        'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
        'weight_decay_rate': 0.0}]

    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)

    loss_list = np.array([])

    for e in tqdm(range(10)):
        for i, batch in enumerate(train_dataloader):
            batch = tuple(t.to(device) for t in batch)
            loss = model(batch[0], token_type_ids=batch[1], attention_mask=batch[2], labels=batch[3])[0]
            print(loss.item())
            np.append(loss_list, loss.item())

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        #torch.save(model, "/content/drive/MyDrive/recipe_data/edge_models/" + f'edge_model_{e+10}.pth')
    return loss_list

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=NUMBER_CLASSES).to(device)
#model = torch.load("/content/drive/MyDrive/recipe_data/edge_models/edge_model_9.pth")
loss_list = train_eval(model, input_ids, token_type_ids, attention_mask, target)

np.save('/content/drive/MyDrive/recipe_data/edge_only_#loss_list.npy',loss_list )

TEST_CLASSES_LIST = list(set(TRAIN_LABELS + TEST_LABELS))
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def pred(model):
    # load model
    #model = torch.load(model_path)

    sentence_list = []

    input_ids, token_type_ids, attention_mask, _ = convert(TEST_ENTITIES, TEST_SENTENCES_LIST, TEST_LABELS) # whatever name_list and label_list
    dataset = Data.TensorDataset(input_ids, token_type_ids, attention_mask)
    loader = Data.DataLoader(dataset, 1, False)

    pred_label = []
    model.eval()
    for i, batch in enumerate(loader):
        batch = tuple(t.to(device) for t in batch)
        with torch.no_grad():
            #logits = model(batch[0], token_type_ids=batch[1], attention_mask=batch[2])[0]
            logits = model(batch[0], token_type_ids=batch[1], attention_mask=batch[2]).logits
            logits = logits.detach().cpu().numpy()
            preds = np.argmax(logits, axis=1).flatten()
            pred_label.extend(preds)
    
    #for i in range(len(pred_label)):
    #    pred_label[i] = TEST_CLASSES_LIST[pred_label[i]]
    
    #pd.DataFrame(data=pred_label, index=range(len(pred_label))).to_csv('pred.csv')
    return pred_label


def pred_train(model):
    # load model
    #model = torch.load(model_path)

    sentence_list = []

    input_ids, token_type_ids, attention_mask, _ = convert(TRAIN_ENTITIES, TRAIN_SENTENCES_LIST, TRAIN_LABELS) # whatever name_list and label_list
    dataset = Data.TensorDataset(input_ids, token_type_ids, attention_mask)
    loader = Data.DataLoader(dataset, 1, False)

    pred_label = []
    model.eval()
    for i, batch in enumerate(loader):
        batch = tuple(t.to(device) for t in batch)
        with torch.no_grad():
            #logits = model(batch[0], token_type_ids=batch[1], attention_mask=batch[2])[0]
            logits = model(batch[0], token_type_ids=batch[1], attention_mask=batch[2]).logits
            logits = logits.detach().cpu().numpy()
            preds = np.argmax(logits, axis=1).flatten()
            pred_label.extend(preds)
    
    #for i in range(len(pred_label)):
    #    pred_label[i] = TEST_CLASSES_LIST[pred_label[i]]
    
    #pd.DataFrame(data=pred_label, index=range(len(pred_label))).to_csv('pred.csv')
    return pred_label

torch.save(model, "/content/drive/MyDrive/recipe_data/edge_only_models.pth")

model = torch.load("/content/drive/MyDrive/recipe_data/edge_only_models.pth")
pred_label = pred(model)

#pred_label
acc = 0
for i in range(len(pred_label)):
  if pred_label[i] == TEST_LABELS[i]:
      acc += 1
print(acc/len(pred_label))

pred_label_train = pred_train(model)

#pred_label
acc = 0
for i in range(len(pred_label_train)):
  if pred_label_train[i] == TRAIN_LABELS[i]:
      acc += 1
print(acc/len(pred_label_train))

def transfer_orignal_label(target_list):
  result = []
  for i in range(len(target_list)):

    if target_list[i] == 0 or target_list[i] == 5:
      result.append(0)
    elif target_list[i] == 2 or target_list[i] == 3:
      result.append(1)
    elif target_list[i] == 13 or target_list[i] == 23:
      result.append(2)
    elif target_list[i] == 24 or target_list[i] == 27:
      result.append(3)

    elif target_list[i] == 16 or target_list[i] == 22:
      result.append(4)

    elif target_list[i] == 6 or target_list[i] == 15:
      result.append(5)

    elif target_list[i] == 14 or target_list[i] == 20:
      result.append(6)

    elif target_list[i] == 18 or target_list[i] == 25:
      result.append(7)

    elif target_list[i] == 4 or target_list[i] == 11:
      result.append(8)

    elif target_list[i] == 8 or target_list[i] == 10:
      result.append(9)

    elif target_list[i] == 21 or target_list[i] == 17:
      result.append(10)

    elif target_list[i] == 7 or target_list[i] == 19:
      result.append(11)

    elif target_list[i] == 9 or target_list[i] == 26:
      result.append(12)
    else:
      result.append(13)
  return result

MULTI_TRAIN_LABELS = edge_train_csv[3].tolist()
MULTI_TEST_LABELS = edge_test_csv[3].tolist()

transfered_label = transfer_orignal_label(MULTI_TRAIN_LABELS + MULTI_TEST_LABELS)

set(transfered_label)

#total_true = transfer_orignal_label(TRAIN_LABELS + TEST_LABELS)
#total_pred = transfer_orignal_label(pred_train_label + pred_label)

total_true = TRAIN_LABELS + TEST_LABELS
total_pred = pred_label_train + pred_label

TP_l = []
FP_l = []
FN_l = []


for k in range(3):
    TP = 0
    FP = 0
    FN = 0
    for i in range(len(total_true)):
        if total_true[i] == k and total_pred[i] == k:
            TP += 1
        elif total_true[i] == k and total_pred[i] != k:
            FN += 1
        elif total_true[i] != k and total_pred[i] == k:
            FP += 1
    TP_l.append(TP)
    FP_l.append(FP)
    FN_l.append(FN)

precisions = []
recalls = []

for i in range(3):
    precisions.append(TP_l[i] / (TP_l[i] + FP_l[i]))
    recalls.append(TP_l[i] / (TP_l[i] + FN_l[i]))

F1_score = []
for i in range(0, len(precisions)):
    F1_score.append(2 * precisions[i] * recalls[i] / (precisions[i] + recalls[i]))

F1_score









#total_true = transfer_orignal_label(TRAIN_LABELS + TEST_LABELS)
#total_pred = transfer_orignal_label(pred_train_label + pred_label)

total_true = TRAIN_LABELS + TEST_LABELS
total_pred = pred_label_train + pred_label

TP_l = []
FP_l = []
FN_l = []


for k in range(13):
    TP = 0
    FP = 0
    FN = 0
    for i in range(len(total_true)):
        if transfered_label[i] == k:
          if total_true[i] == total_pred[i] and total_true[i] in [1, 2]:
              TP += 1
          elif total_true[i] != total_pred[i] and total_true[i] in [1, 2]:
              FN += 1
          elif total_true[i] != total_pred[i] and total_true[i] not in [1, 2]:
              FP += 1
    TP_l.append(TP)
    FP_l.append(FP)
    FN_l.append(FN)

precisions = []
recalls = []

for i in range(13):
    precisions.append(TP_l[i] / (TP_l[i] + FP_l[i]))
    recalls.append(TP_l[i] / (TP_l[i] + FN_l[i]))

F1_score = []
for i in range(0, len(precisions)):
    F1_score.append(2 * precisions[i] * recalls[i] / (precisions[i] + recalls[i]))

F1_score















#total_true = transfer_orignal_label(TRAIN_LABELS + TEST_LABELS)
#total_pred = transfer_orignal_label(pred_train_label + pred_label)

total_true = TEST_LABELS
total_pred = pred_label

TP_l = []
FP_l = []
FN_l = []

for k in range(3):
    TP = 0
    FP = 0
    FN = 0
    for i in range(len(total_true)):
        if total_true[i] == k and total_pred[i] == k:
            TP += 1
        elif total_true[i] == k and total_pred[i] != k:
            FN += 1
        elif total_true[i] != k and total_pred[i] == k:
            FP += 1
    TP_l.append(TP)
    FP_l.append(FP)
    FN_l.append(FN)

precisions = []
recalls = []

for i in range(3):
    precisions.append(TP_l[i] / (TP_l[i] + FP_l[i]))
    recalls.append(TP_l[i] / (TP_l[i] + FN_l[i]))

F1_score = []
for i in range(0, len(precisions)):
    F1_score.append(2 * precisions[i] * recalls[i] / (precisions[i] + recalls[i]))

F1_score





































